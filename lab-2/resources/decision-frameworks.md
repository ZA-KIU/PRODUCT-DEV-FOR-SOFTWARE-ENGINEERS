# Decision Frameworks for Lab 2

**Purpose:** This document contains frameworks, rubrics, and checklists to help your team make decisions during Lab 2 and ensure quality work.

---

## Table of Contents

1. [The 4 Filters Framework - Problem Selection](#1-the-4-filters-framework)
2. [ICP Specificity Framework](#2-icp-specificity-framework)
3. [Interview Script Quality Rubric](#3-interview-script-quality-rubric)
4. [Mom Test Compliance Checker](#4-mom-test-compliance-checker)
5. [Team Decision-Making Framework](#5-team-decision-making-framework)
6. [Synthesis Preparation Guide](#6-synthesis-preparation-guide)
7. [Red Flags - When to Pivot](#7-red-flags---when-to-pivot)

---

## 1. The 4 Filters Framework

**Use this to evaluate and select your team's problem.**

### Overview

Every problem idea should be scored on 4 dimensions, each rated 0-5. Total score out of 20. Problems scoring 15+ are strong candidates. Problems scoring below 12 should be reconsidered.

---

### Filter 1: Hair on Fire Test (0-5 points)

**Question:** How painful is this problem? Will people actively seek solutions?

**Scoring Criteria:**

**5 points - Severe Pain:**
- Problem causes significant distress or cost
- People complain about it frequently
- People have built elaborate workarounds
- Emotional language used: "frustrated," "hate," "can't stand"
- Problem prevents people from achieving important goals
- People would actively seek and pay for solutions

**Example:** Students miss critical deadlines due to LMS notification overload, resulting in lost grade points and high stress.

**3 points - Moderate Pain:**
- Problem is annoying and inconvenient
- People mention it when asked but don't bring it up unprompted
- Some basic workarounds exist
- People tolerate it but wish it were better
- Would use a solution if it appeared, but not desperately

**Example:** Finding old lecture slides takes 5 extra minutes, which is annoying but manageable.

**1 point - Mild Pain:**
- Minor inconvenience
- People barely notice it
- No workarounds needed
- "Nice-to-have" territory
- People wouldn't change current behavior for a solution

**Example:** Course materials could be organized slightly better, but current system is "fine."

**0 points - No Pain:**
- Not actually a problem
- People don't experience any difficulty

---

**Quick Assessment Questions:**

Ask yourself these questions to score this filter:

1. **When you mention this problem to people, what's their reaction?**
   - "OMG yes, I hate that!" → High score
   - "Yeah, that's a bit annoying" → Medium score
   - "Huh, never really thought about it" → Low score

2. **Have people built workarounds or hacks?**
   - Complex workarounds (WhatsApp groups, spreadsheets, manual processes) → High score
   - Simple workarounds (just ask someone) → Medium score
   - No workarounds needed → Low score

3. **What's the emotional intensity?**
   - Strong negative emotions (frustration, stress, anger) → High score
   - Mild annoyance → Medium score
   - No emotional response → Low score

4. **What are the consequences of the problem?**
   - Significant: Lost money, missed deadlines, failed goals → High score
   - Moderate: Wasted time, mild inconvenience → Medium score
   - Minimal: Barely noticeable impact → Low score

---

### Filter 2: Access Test (0-5 points)

**Question:** Can we find 10+ people who experience this problem at KIU?

**Scoring Criteria:**

**5 points - Very Easy Access:**
- Can identify 20-30+ people experiencing this problem
- People are all around you (classmates, dorm mates, same building)
- Multiple specific locations where you can find them
- Can reach them through existing channels (class groups, clubs)
- No gatekeepers to go through

**Example:** Second-year CS students with group projects - can find 50+ in your own classes.

**3 points - Moderate Access:**
- Can probably find 10-15 people with effort
- Need to use referrals or network a bit
- Some people experiencing this on campus
- Might need to post in groups or ask for introductions
- Takes some work but definitely achievable

**Example:** Students who commute from specific neighborhood - need to ask around but can find them.

**1 point - Difficult Access:**
- Very hard to find people experiencing this
- Highly specific niche segment
- Might need to go off-campus
- May not hit 10 interviews easily
- Gatekeepers or barriers to access

**Example:** Students who have specific medical condition - very hard to identify and reach.

**0 points - No Access:**
- Can't find people at KIU
- Segment doesn't exist in your reach
- Would need to leave Kutaisi to find them

---

**Quick Assessment Questions:**

1. **Can you name 5 specific people right now who experience this?**
   - Yes, easily → High score
   - Maybe, with some thought → Medium score
   - No, would need to search → Low score

2. **Can you name 5 specific physical or digital locations where you'd find them?**
   - Yes, with details → High score
   - Yes, but vague → Medium score
   - No → Low score

3. **Can you reach 10 people this week?**
   - Definitely yes → High score
   - Probably, with effort → Medium score
   - Unlikely → Low score

4. **Are there any gatekeepers?**
   - No gatekeepers, direct access → High score
   - Some referrals needed → Medium score
   - Multiple gatekeepers or permissions needed → Low score

---

### Filter 3: Feasibility Test (0-5 points)

**Question:** Can we build a meaningful MVP in 8-10 weeks with our skills?

**Scoring Criteria:**

**5 points - Definitely Feasible:**
- Solution requires skills you already have
- Standard web/mobile app architecture
- No specialized hardware or infrastructure
- Can use existing libraries and frameworks
- Similar apps exist that you can learn from
- Reasonable scope for 8-10 weeks

**Example:** A web app with user auth, database, simple UI - standard CRUD app with one novel feature.

**3 points - Challenging but Doable:**
- Requires learning some new technologies (but learnable in time)
- Involves some technical complexity but manageable
- Might need to simplify scope but core value is achievable
- No hardware or specialized infrastructure needed
- 8-10 weeks is tight but possible with focus

**Example:** Real-time collaboration features, integration with existing APIs, some algorithm work.

**1 point - Very Difficult:**
- Requires significant new technical learning
- Complex architecture or algorithms
- Hardware components needed
- Infrastructure requirements unclear
- 8-10 weeks is probably not enough time
- Would need to cut scope significantly

**Example:** Machine learning model requiring training data collection, custom hardware, complex distributed systems.

**0 points - Not Feasible:**
- Requires skills far beyond current level
- Needs resources you don't have
- Time requirement exceeds semester
- Technical barriers are insurmountable

---

**Quick Assessment Questions:**

1. **MVP Technical Requirements Breakdown:**

List what you'd need to build:
- [ ] Frontend (web/mobile)
- [ ] Backend/API
- [ ] Database
- [ ] Authentication
- [ ] Third-party integrations
- [ ] Algorithms/Logic
- [ ] Hardware
- [ ] Other: ___________

**For each item, answer:**
- Do we have these skills? (Yes / Need to learn / Don't have)
- Can we learn in 2-3 weeks? (Yes / Maybe / No)
- Are there existing libraries? (Yes / Some / No)

2. **Scope Check:**
   - Can you describe MVP in one sentence? → Good sign
   - Needs multiple paragraphs to explain → Scope too big
   - "Like [existing app] but for [specific use case]" → Usually feasible
   - "Revolutionary new approach to..." → Scope too ambitious

3. **Technical Risk Assessment:**
   - No novel technical challenges → High score
   - 1-2 new things to learn → Medium score
   - 3+ new technologies or complex algorithms → Low score

4. **Time Reality Check:**
   - 8-10 weeks = ~60-80 hours of work per person
   - Can you build core functionality in that time? → High score
   - Would need to cut major features? → Medium score
   - Not nearly enough time? → Low score

---

### Filter 4: Team Energy Test (0-5 points)

**Question:** How motivated is the team to work on this problem for 15 weeks?

**Scoring Criteria:**

**5 points - Everyone is Excited:**
- Multiple team members proposed similar problems
- Everyone lights up when discussing it
- People are already brainstorming solutions
- Team members have personally experienced the pain
- Natural enthusiasm and interest from all
- No one is faking excitement

**Example:** All team members have struggled with group project coordination, everyone nods when problem is described.

**3 points - Mixed Enthusiasm:**
- Some people excited, others neutral
- No strong opposition, just varying levels of interest
- Some team members haven't experienced the problem themselves
- Willing to work on it but not passionate
- Respectful disagreement about priority

**Example:** Two people love the idea, two people think it's "fine" but not their top choice.

**1 point - One Person's Problem:**
- Only one team member cares about this
- Others are indifferent or skeptical
- Hard to generate discussion about the problem
- People change subject quickly
- Feels forced or obligatory

**Example:** One person keeps pitching their idea while others are clearly not interested.

**0 points - Team Opposition:**
- People actively don't want to work on this
- Multiple team members expressed concerns
- Bad group dynamic around this problem

---

**Quick Assessment Questions:**

1. **Gut Check - Rate individually then compare:**

Each team member secretly rates: "On a scale of 1-10, how excited am I to work on this problem for 15 weeks?"

- All ratings 7-10 → High score
- Mix of 5-8 → Medium score
- Any ratings below 5 → Low score

2. **The "Why" Test:**

Ask each team member: "Why should we work on this problem?"

- Multiple enthusiastic, detailed answers → High score
- Mix of "seems interesting" and "I guess it's fine" → Medium score
- Shrugs or "because we have to pick something" → Low score

3. **The Personal Connection Test:**

How many team members have personally experienced this problem?

- All or most → High score
- Some → Medium score
- Only one → Low score

4. **The 15-Week Test:**

Imagine it's Week 12, you're tired, midterms are happening, and you still need to work on this project. Will you be motivated?

- Yes, still excited → High score
- Maybe, hope so → Medium score
- Probably would rather be doing something else → Low score

---

### Scoring Summary Table

Create this table during your team discussion:

| Problem | Hair on Fire | Access | Feasibility | Team Energy | TOTAL |
|---------|--------------|--------|-------------|-------------|-------|
| Problem A | [0-5] | [0-5] | [0-5] | [0-5] | [/20] |
| Problem B | [0-5] | [0-5] | [0-5] | [0-5] | [/20] |
| Problem C | [0-5] | [0-5] | [0-5] | [0-5] | [/20] |

**Interpretation:**

- **17-20 points:** Excellent candidate - pursue this
- **15-16 points:** Strong candidate - good choice
- **12-14 points:** Acceptable but has weaknesses - discuss thoroughly
- **Below 12:** Weak candidate - reconsider or choose different problem

**Tie-Breaker Rules:**

If two problems score within 2 points of each other:

1. **Prioritize Hair on Fire:** If one has significantly higher pain (3+ vs 2), choose that one
2. **Prioritize Team Energy:** If team energy differs significantly, choose the one team is more excited about
3. **Consider Access + Feasibility combined:** If one is much easier to execute (7-8 points combined) vs harder (5-6 points), choose the easier one
4. **Vote:** If still tied, simple majority vote

---

## 2. ICP Specificity Framework

**Use this to ensure your ICP is specific enough to be useful.**

### The Specificity Test

Your ICP should pass all 5 tests below:

---

### Test 1: The Stranger Test

**Question:** Could a stranger use your ICP description to identify the right person?

**Method:**
Read your ICP description to someone outside your team (a friend, roommate, anyone). Then ask them: "If you were on campus, could you find someone matching this description?"

**Pass:** They can describe exactly who to look for and where to find them  
**Fail:** They say "uh, that could be anyone" or "I'm not sure where to find them"

**Examples:**

❌ **TOO VAGUE:** "Students at KIU"
- Stranger says: "Um, there are thousands of students"

✅ **SPECIFIC:** "Third-year Computer Science students taking Database Systems (K-301, Tuesday/Thursday 2 PM) who live on campus and have 5+ concurrent courses"
- Stranger says: "Oh, I should go to that class on Tuesday, arrive 10 minutes before it ends, and ask students about their course load"

---

### Test 2: The 10-Person Test

**Question:** Can you identify at least 10 specific people or specific ways to reach 10 people who match your ICP?

**Method:**
Right now, without doing any research:
- Can you name 5 specific people who match your ICP?
- Can you name 5 specific places (physical or digital) where you'd find 10+ people matching your ICP?

**Pass:** Yes to both  
**Fail:** Can't do one or both

**Exercise:**

List 5 people or 5 places right now:

1. ____________________
2. ____________________
3. ____________________
4. ____________________
5. ____________________

If you struggled, your ICP is too vague or too narrow.

---

### Test 3: The Homogeneity Test

**Question:** Do people in your ICP have similar experiences with the problem?

**Method:**
Imagine three different people who match your ICP description. Walk through their daily experience with your problem.

**Pass:** All three would describe the problem very similarly  
**Fail:** The three people would describe very different problems or have very different experiences

**Examples:**

❌ **TOO BROAD:** "Students"
- First-year student: Barely has any assignments yet
- Third-year student: Has complex group projects
- Fourth-year student: Focused on thesis, different problems entirely
- **FAIL:** These are not homogeneous experiences

✅ **SPECIFIC:** "Third-year CS students with 5+ courses"
- Student A: Has Database, Web Dev, OS projects overlapping
- Student B: Has same courses, similar coordination issues
- Student C: Also has multiple overlapping projects
- **PASS:** Similar experiences with coordination chaos

---

### Test 4: The Screener Test

**Question:** Can you write 2-3 yes/no questions that definitively determine if someone matches your ICP?

**Method:**
Write screener questions. If someone answers YES to all questions, they're in your ICP. If NO to any question, they're not.

**Pass:** Clear yes/no questions that create sharp boundary  
**Fail:** Questions are subjective or create fuzzy boundaries

**Examples:**

❌ **FUZZY BOUNDARIES:**
1. "Do you consider yourself a student?" (What if they're part-time? What if they're about to graduate?)
2. "Do you have problems with the LMS?" (Subjective - what counts as "problems"?)

✅ **SHARP BOUNDARIES:**
1. "Are you currently enrolled as a second or third-year student in the CS program?" (YES/NO)
2. "Are you currently taking 3 or more courses that have group project requirements?" (YES/NO)
3. "Are you actively working on at least one group project right now?" (YES/NO)

---

### Test 5: The Goldilocks Test

**Question:** Is your ICP not too broad, not too narrow, but just right?

**Method:**
Answer these questions:

**Too Broad Check:**
- [ ] Does your ICP include people with very different contexts or experiences?
- [ ] Would two random people from your ICP describe the problem differently?
- [ ] Does your ICP have more than 100 people at KIU?

If YES to 2+ of these → Too broad, add more specificity

**Too Narrow Check:**
- [ ] Can you find fewer than 10 people matching your ICP at KIU?
- [ ] Does your ICP require very rare or specific circumstances?
- [ ] Would you need to leave Kutaisi to find people?

If YES to any of these → Too narrow, broaden slightly

**Just Right Check:**
- [ ] Can you find 15-30 people matching your ICP at KIU?
- [ ] Do people in your ICP have similar experiences with the problem?
- [ ] Can you describe your ICP in 1-2 sentences clearly?

If YES to all → Just right!

---

### ICP Specificity Checklist

Use this checklist to verify your ICP quality:

**Demographics:**
- [ ] Specified year/level (not just "students")
- [ ] Specified major/program (if relevant)
- [ ] Specified other relevant demographics

**Behaviors:**
- [ ] Described how often they experience the problem
- [ ] Described what they currently do
- [ ] Described their tech comfort level

**Context:**
- [ ] Specified when problem occurs
- [ ] Specified where problem occurs
- [ ] Specified what triggers the problem

**Constraints:**
- [ ] Identified their time constraints
- [ ] Identified their resource constraints
- [ ] Identified their authority constraints

**Finding Them:**
- [ ] Listed 5+ specific places to find them
- [ ] Each location is detailed (not vague)
- [ ] Created 2-3 screener questions

**Quality Gates:**
- [ ] Passes Stranger Test
- [ ] Passes 10-Person Test
- [ ] Passes Homogeneity Test
- [ ] Passes Screener Test
- [ ] Passes Goldilocks Test

---

## 3. Interview Script Quality Rubric

**Use this to evaluate whether your interview script is ready to use.**

### Overall Structure (15 points)

| Criterion | Points | What to Check |
|-----------|--------|---------------|
| **All 5 sections present** | 3 pts | Opening, Context, Problem Deep Dive, Current Solutions, Closing |
| **Appropriate time allocation** | 3 pts | Section 3 (Problem Deep Dive) is longest (10-15 min of 25 min total) |
| **Logical flow** | 3 pts | Moves from broad → specific → deep, transitions make sense |
| **Natural language** | 3 pts | Sounds conversational, not like a survey |
| **Customized for your problem** | 3 pts | Specific to your problem, not generic template |

**Scoring:**
- 13-15 points: Excellent structure
- 10-12 points: Good, minor improvements needed
- Below 10: Needs significant revision

---

### Mom Test Compliance (25 points)

| Criterion | Points | What to Check |
|-----------|--------|---------------|
| **No hypothetical questions** | 5 pts | Zero "would you" or "if we built" questions |
| **No opinion questions** | 5 pts | Zero "do you think" or "what's your opinion" questions |
| **No leading questions** | 5 pts | Zero questions that imply the answer |
| **Asks about past behavior** | 5 pts | Questions focus on specific recent experiences |
| **No solution pitching** | 5 pts | Script doesn't mention or describe your solution |

**How to check:**
Go through every question in your script. Mark any that violate Mom Test principles. 

**Acceptable violations:**
- Section 5 (Closing) can ask "would you try" for beta testing - this is fine
- Context questions can be present-tense ("How many courses are you taking?")

**Unacceptable violations:**
- Anything in Section 3 that asks about future or opinions
- Any question that pitches your solution
- Any question that leads them to a specific answer

**Scoring:**
- 23-25 points: Excellent Mom Test compliance
- 20-22 points: Good, minor violations that can be fixed
- Below 20: Needs significant revision, multiple violations

---

### Five Whys Depth (15 points)

| Criterion | Points | What to Check |
|-----------|--------|---------------|
| **Has "Why?" questions** | 5 pts | At least 3 questions asking "Why?" or "What caused...?" |
| **Has "Tell me more" prompts** | 5 pts | At least 2 "Tell me more about that" follow-ups |
| **Explores root causes** | 5 pts | Questions dig beyond surface symptoms to underlying causes |

**How to check:**
Count follow-up questions in Section 3:
- Questions asking "Why?" or variations: ____
- Questions asking "Tell me more": ____
- Questions exploring consequences or deeper causes: ____

**Total should be at least 5-7 follow-up questions.**

**Scoring:**
- 13-15 points: Excellent depth
- 10-12 points: Good, could add 1-2 more follow-ups
- Below 10: Too shallow, needs more depth questions

---

### Specificity (15 points)

| Criterion | Points | What to Check |
|-----------|--------|---------------|
| **Opening story question is specific** | 5 pts | Asks about "last time" or specific recent instance, not generic |
| **Questions probe for details** | 5 pts | Follow-ups ask for specific examples, not estimates |
| **Avoids vague questions** | 5 pts | No questions like "How often?" without anchoring to specific timeframe |

**Examples:**

❌ **VAGUE:** "How often does this happen?"  
✅ **SPECIFIC:** "Think about this past week, Monday through today. How many times did this happen?"

❌ **VAGUE:** "Do you find this frustrating?"  
✅ **SPECIFIC:** "When that happened, how did you feel? What did you do next?"

**Scoring:**
- 13-15 points: Very specific, forces detailed answers
- 10-12 points: Mostly specific, a few vague questions
- Below 10: Too many vague questions

---

### Emotional & Consequence Exploration (15 points)

| Criterion | Points | What to Check |
|-----------|--------|---------------|
| **Asks about feelings** | 5 pts | At least 2 questions about emotional impact |
| **Asks about consequences** | 5 pts | At least 2 questions about impact/results |
| **Explores significance** | 5 pts | Questions uncover why problem matters to them |

**Look for questions like:**
- "How did that make you feel?"
- "What was the impact of that?"
- "Did that affect anything else?"
- "Why does that matter to you?"

**Scoring:**
- 13-15 points: Strong emotional/consequence exploration
- 10-12 points: Some exploration, could be deeper
- Below 10: Lacks emotional or consequence questions

---

### Current Solutions Section (15 points)

| Criterion | Points | What to Check |
|-----------|--------|---------------|
| **Asks what they do now** | 5 pts | Clear question about current behavior |
| **Explores existing tools** | 5 pts | Questions about what tools/methods they use |
| **Probes pain points in current solutions** | 5 pts | Questions about what doesn't work |

**Key questions to include:**
- "What do you currently do when [problem] happens?"
- "Have you tried other solutions?"
- "What works/doesn't work about your current approach?"

**Scoring:**
- 13-15 points: Thorough current solutions exploration
- 10-12 points: Basic current solutions questions
- Below 10: Missing current solutions exploration

---

### Total Score

| Category | Points Possible | Your Score |
|----------|-----------------|------------|
| Overall Structure | 15 | ____ |
| Mom Test Compliance | 25 | ____ |
| Five Whys Depth | 15 | ____ |
| Specificity | 15 | ____ |
| Emotional & Consequence Exploration | 15 | ____ |
| Current Solutions Section | 15 | ____ |
| **TOTAL** | **100** | **____** |

**Interpretation:**

- **90-100:** Excellent script, ready to use
- **80-89:** Good script, minor revisions recommended
- **70-79:** Acceptable but needs improvements before use
- **Below 70:** Significant revisions needed, review examples and templates

---

## 4. Mom Test Compliance Checker

**Use this quick reference to check every question in your script.**

### The Three Rules

Every question in your interview script (especially Section 3) must follow these three rules:

**Rule 1: Talk about their life, not your idea**  
**Rule 2: Ask about specifics in the past, not generics or opinions about the future**  
**Rule 3: Talk less, listen more**

---

### Question-by-Question Checker

For each question in your script, ask:

#### Does this question violate Mom Test?

**🚫 VIOLATION if the question:**
- Asks about the future ("would you...")
- Asks for opinions ("do you think...")
- Asks about your solution ("if we built...")
- Is leading ("don't you find it frustrating when...")
- Is generic ("how often...") without anchoring to specific timeframe
- Puts words in their mouth

**✅ COMPLIANT if the question:**
- Asks about past specific experience
- Asks for a story ("tell me about the last time...")
- Asks for details ("what happened next?")
- Explores their current behavior
- Digs deeper into what they said ("why was that?")

---

### Quick Reference Examples

| ❌ Mom Test Violation | ✅ Mom Test Compliant |
|----------------------|----------------------|
| Would you use an app that...? | Tell me about the last time you tried to... |
| Do you think this is a problem? | Walk me through what happened when... |
| If we built X, would you pay $5? | What do you currently do? What does that cost you? |
| How often does this happen? | Think about this past week. How many times did this happen? Walk me through one specific time. |
| Don't you find it frustrating when...? | How did you feel when that happened? |
| What features would you want? | What have you tried? What works/doesn't work about it? |
| Is this important to you? | What was the impact? Did it affect anything else? |
| Would this solve your problem? | What would need to be different to make this easier? |

---

### The 5-Second Test

For any question you're unsure about, apply this test:

**1. Does it ask about the future?**
- If YES → Violation
- If NO → Continue to #2

**2. Does it ask for an opinion?**
- If YES → Violation
- If NO → Continue to #3

**3. Does it mention your solution?**
- If YES → Violation
- If NO → Continue to #4

**4. Does it ask about a specific past experience?**
- If YES → Probably compliant
- If NO → Continue to #5

**5. Does it lead them to a specific answer?**
- If YES → Violation
- If NO → Probably compliant

---

## 5. Team Decision-Making Framework

**Use this when your team needs to make a decision and there's disagreement.**

### Decision Type Classification

First, classify what type of decision this is:

**Type A: Reversible Decision** (Can easily change later)
- Examples: Which communication channel to use, meeting schedule, role assignments
- Impact: Low to medium
- Approach: Move fast, decide quickly, iterate if needed

**Type B: Sticky Decision** (Hard to change later)
- Examples: Which problem to pursue, ICP definition, technical architecture
- Impact: High
- Approach: Take time, discuss thoroughly, ensure alignment

---

### Decision Process by Type

#### For Type A (Reversible) Decisions:

**Process:**
1. Discuss for 10-15 minutes
2. If consensus doesn't emerge quickly, vote
3. Majority wins
4. Document decision
5. Revisit in 2 weeks if not working

**Example:**
"Should we use WhatsApp or Slack for team communication?"
- 15-minute discussion
- Vote: 3 prefer WhatsApp, 1 prefers Slack
- Decision: WhatsApp
- Revisit: If it's not working in 2 weeks, we'll switch

---

#### For Type B (Sticky) Decisions:

**Process:**
1. Frame the decision clearly
2. Each person shares perspective (5 min each)
3. Discuss for 30+ minutes
4. Seek consensus
5. If no consensus after thorough discussion, use tie-breaker method
6. Document decision and rationale

**Example:**
"Which problem should we pursue?"
- Frame: "We need to select ONE problem for the semester"
- Each person pitches their top choice (5 min each)
- Discuss using 4 Filters framework (30-45 min)
- Seek consensus
- If tied, use scoring + vote
- Document: Which problem, why, what we're committing to

---

### The Disagree and Commit Principle

**When to use:** After team makes a decision that you don't personally agree with

**What it means:**
1. You share your perspective fully during discussion
2. Team makes a decision (even if it's not your preference)
3. You **commit to the decision** as if it were your own
4. You don't undermine it or half-heartedly support it
5. You give it your full effort

**Why it matters:**
- Team needs unity to succeed
- Debating forever is worse than making a "good enough" decision
- Most decisions can be revisited if they're truly wrong

**Example:**
"I wanted to pursue Problem A, but the team scored Problem B higher using the 4 Filters. I disagree with the scoring, but I'm committing to Problem B. I'll put my full energy into it."

---

### Escalation Path

If team absolutely cannot reach a decision:

**Step 1: Take a break** (30 minutes or overnight)
- Sometimes fresh perspective helps
- Reduces emotional tension

**Step 2: Reframe the decision**
- Are we asking the right question?
- Is there a third option we haven't considered?
- Can we simplify the decision?

**Step 3: Bring in outside perspective**
- Ask another team for their input
- Explain both sides to an outsider
- See what they think

**Step 4: Use role-based authority**
- If decision relates to technology → Tech Lead decides
- If decision relates to user research → Discovery Lead decides
- If decision relates to process/timeline → Program Lead decides

**Step 5: Escalate to instructor**
- Only if Steps 1-4 have failed
- Document: What's the decision? What perspectives exist? Why can't you decide?
- Instructor provides guidance or makes final call

---

## 6. Synthesis Preparation Guide

**Use this in Week 5 when you're ready to synthesize interview findings.**

This is a preview of what you'll do in Week 5. Understanding it now helps you know what to capture during interviews.

---

### What is Synthesis?

**Synthesis = Turning 10+ individual interview logs into patterns and a problem statement**

You'll do this after completing all interviews. But knowing the synthesis process NOW helps you capture the right data DURING interviews.

---

### Pre-Synthesis Checklist

Before beginning synthesis, ensure you have:

- [ ] 10+ completed interview logs
- [ ] Each log has 3+ verbatim quotes
- [ ] Each log has specific story/example
- [ ] Each log has JTBD statement
- [ ] Each log has current solutions documented
- [ ] Each log has consequences/impact documented

If you're missing any of these, your synthesis will be weak. Go back and complete logs.

---

### Synthesis Process Overview

**Total Time:** 4-5 hours as a team  
**When:** Week 5, after completing all interviews  
**Output:** Problem statement backed by evidence

**The 4 Steps:**

1. **Gather Evidence** (30 min)
2. **Affinity Mapping** (60 min)
3. **Pattern Recognition** (60 min)
4. **Problem Statement Creation** (45 min)

---

### Step 1: Gather Evidence (30 minutes)

**What:** Review all interview logs as a team

**How:**
- Meet as full team with all 10 interview logs
- Each person reads their 2-3 interview logs aloud (key points)
- Focus on: Verbatim quotes, specific stories, pain points, current solutions
- Don't analyze yet - just get fresh memory of all data

**Output:** Whole team has fresh understanding of all interviews

---

### Step 2: Affinity Mapping (60 minutes)

**What:** Group similar insights together to find patterns

**Tools Needed:**
- Sticky notes (physical or digital - Miro, FigJam, Jamboard)
- Whiteboard or large wall space
- All interview logs

**Process:**

**A. Extract Insights (20 min):**
- Go through each interview log
- Write each key quote or insight on ONE sticky note
- One insight per sticky note
- Use exact quotes when possible
- Aim for 50-100 sticky notes total across all interviews

**B. Group Similar Insights (30 min):**
- Start sticking notes on wall/board randomly
- Move notes with similar themes close together
- Don't overthink categories yet - let natural groups emerge
- Keep moving notes until clear clusters form

**C. Label Clusters (10 min):**
- Once clusters are stable, label each cluster
- Label should describe the theme (e.g., "WhatsApp message overload," "Last-minute panic," "Unclear task ownership")
- Some notes might not fit anywhere - that's okay, set them aside

**Output:** Visual map of themes with supporting evidence

---

### Step 3: Pattern Recognition (60 minutes)

**What:** Identify what patterns are strongest and most important

**Process:**

**A. Count Frequency (15 min):**
For each cluster, count:
- How many interviews mentioned this theme?
- How many quotes support this theme?

Create frequency table:
Theme# Interviews Mentioned# Supporting QuotesWhatsApp buried messages8/1012Last-minute consolidation panic9/1015Unclear who's doing what7/1010

**B. Assess Pain Intensity (15 min):**
For each cluster, assess emotional intensity:
- What emotional words did interviewees use?
- How strongly did they feel about this?
- Rate 1-5: 1 = mild annoyance, 5 = severe pain

**C. Identify Root Causes (20 min):**
For top 3-5 themes, ask:
- Is this a symptom or a root cause?
- What's the underlying reason this happens?
- Use Five Whys on the pattern itself

**Example:**
- Surface: "Messages get buried in WhatsApp"
- Why? "Too many messages in group chat"
- Why? "Every small update triggers a message"
- Why? "No other place to post updates"
- Root Cause: "No persistent, visible status tracking"

**D. Rank Patterns (10 min):**
Rank patterns by:
1. Frequency (how many mentioned it)
2. Intensity (how painful is it)
3. Root cause vs symptom (root causes rank higher)

**Output:** Ranked list of top 3-5 pain points with evidence

---

### Step 4: Problem Statement Creation (45 minutes)

**What:** Write evidence-based problem statement

**Formula:**
[Specific segment] experiences [specific problem] in [context] because [root cause],
resulting in [measurable consequence].
Evidence: [cite X interviews with specific details].

**Process:**

**A. Draft Each Component (25 min):**

**Specific Segment:**
From your ICP: "Third-year CS students taking 5+ courses with concurrent group projects"

**Specific Problem:**
From your top-ranked pattern: "Struggle to maintain visibility into team members' task status"

**Context:**
From interview patterns: "When working asynchronously across multiple projects"

**Root Cause:**
From root cause analysis: "No single source of truth for task ownership and progress across disconnected tools (WhatsApp, Google Drive, GitHub)"

**Measurable Consequence:**
From interview quotes: "Spend 2-4 hours per week on coordination overhead, experience last-minute integration scrambles, duplicate work, and high stress near deadlines"

**Evidence:**
"Documented across 10 interviews: 9/10 mentioned last-minute consolidation panic, 8/10 mentioned message overload, 7/10 mentioned unclear task ownership. [Specific quotes from interviews 1,3,4,5,6,7,8,9,10]"

**B. Assemble Problem Statement (10 min):**

Put it all together:

"Third-year CS students taking 5+ concurrent courses struggle to maintain visibility into team members' task status when working asynchronously because there's no single source of truth for task ownership and progress across disconnected tools (WhatsApp, Google Drive, GitHub), resulting in 2-4 hours per week spent on coordination overhead, last-minute integration scrambles, duplicated work, and high stress levels near deadlines. Evidence documented across 10 interviews: 9/10 mentioned last-minute consolidation panic (quotes from interviews 1,2,3,4,5,6,8,9,10), 8/10 mentioned message overload in WhatsApp (quotes from interviews 1,3,4,5,6,7,8,10), 7/10 mentioned unclear task ownership (quotes from interviews 1,3,4,5,7,9,10)."

**C. Quality Check (10 min):**

Does your problem statement pass these tests?

- [ ] **Specificity:** Would a stranger know exactly who you're talking about?
- [ ] **Evidence:** Can you point to 8+ interviews supporting this?
- [ ] **Root Cause:** Did you identify the underlying cause, not just symptom?
- [ ] **Consequences:** Can you quantify the impact with numbers from interviews?
- [ ] **Actionability:** Does this give clear direction for solutions?

**Output:** Final problem statement ready for submission

---

### What to Capture During Interviews (Now)

Knowing the synthesis process above, here's what you need to capture NOW during interviews:

**Must-Haves:**
1. **3+ verbatim quotes per interview** - You'll need these for affinity mapping
2. **Specific story with details** - Frequency counts come from specific mentions
3. **Emotional language** - Needed for pain intensity assessment
4. **Current solutions** - Needed to understand root causes
5. **Consequences with numbers** - Needed for measurable impact (time wasted, money lost, etc.)

**Don't worry about:**
- Perfect writing - notes are for your team
- Complete sentences - fragments are fine if you understand them
- Beautiful formatting - focus on content

**The key:** Capture raw data now. You'll process and analyze in Week 5.

---

## 7. Red Flags - When to Pivot

**Use this to recognize when your problem choice isn't working out.**

### Early Red Flags (Week 4 - During Interviews)

If you see 3+ of these red flags during your first 5 interviews, consider pivoting:

**🚩 Red Flag 1: People Don't Experience the Problem**
- You describe the problem, they say "huh, that's never happened to me"
- You ask about last time, they can't remember any instance
- They struggle to give you specific examples

**What to do:** 
- Maybe your ICP is wrong (you're interviewing wrong people)
- Maybe the problem isn't real
- After 5 interviews with no confirmation → serious concern

---

**🚩 Red Flag 2: The Problem Is Mild, Not Painful**
- People say "yeah, that's a bit annoying" with flat affect
- No emotional language (frustrated, stressed, hate, etc.)
- No elaborate workarounds built
- They shrug when you ask about impact
- Pain rating below 4/10 consistently

**What to do:**
- This is a "nice-to-have" not a "must-have"
- People won't adopt your solution
- After 5 interviews with low pain → pivot

---

**🚩 Red Flag 3: Current Solutions Work Well Enough**
- People have found solutions they're happy with
- They say "I just use X and it's fine"
- No complaints about their current approach
- They're not seeking alternatives

**What to do:**
- You're late to market
- Switching costs are high
- After 5 interviews with good current solutions → pivot

---

**🚩 Red Flag 4: You're Pitching, Not Learning**
- You find yourself explaining the problem to them
- You're leading them to say what you want to hear
- You're describing your solution mid-interview
- You're defending the problem when they're skeptical

**What to do:**
- Your bias is contaminating research
- Go back to Mom Test principles
- If you can't stop pitching → the problem might not be real

---

**🚩 Red Flag 5: Interviews Feel Like Pulling Teeth**
- Interviewees give one-word answers
- No stories, no details, no examples
- They check their phone, seem bored
- Interviews end in 10 minutes because there's nothing to talk about

**What to do:**
- They don't care about this problem
- Not a real pain point for them
- After 3-5 difficult interviews → pivot

---

### The 5-Interview Checkpoint

**After completing 5 interviews, have a team meeting:**

**Answer these questions:**

1. **Do 4+ of our 5 interviews confirm the problem exists?**
   - If NO → Pivot or redefine ICP

2. **Do 4+ of our 5 interviews show strong pain (7+ out of 10)?**
   - If NO → Pivot to more painful problem

3. **Are we hearing consistent stories and quotes?**
   - If NO → Problem might be too broad or ICP wrong

4. **Are people currently struggling without good solutions?**
   - If NO → Current solutions work, no market

5. **Can we build something meaningfully better than current solutions?**
   - If NO → Technical feasibility issue

**Decision:**
- 5 YES → Great! Continue with remaining 5 interviews
- 3-4 YES → Okay, but discuss concerns. Maybe adjust ICP slightly
- 0-2 YES → Pivot now. Don't waste time on remaining interviews

---

### How to Pivot

If you decide to pivot after 5 interviews:

**Step 1: Learn from Failures (30 min)**
- What did we learn from these 5 interviews?
- Why wasn't this problem working?
- What patterns did we see?

**Step 2: Review Other Problems (30 min)**
- Go back to your original problem list
- Consider your second-choice problem
- Apply 4 Filters again with new information

**Step 3: Redefine ICP (30 min)**
- Was the problem right but ICP wrong?
- Should we narrow or broaden ICP?
- Draft new ICP if needed

**Step 4: Create New Script (60 min)**
- Write new interview script for new problem/ICP
- Practice with team
- Get instructor feedback

**Step 5: Resume Interviews (Week 4-5)**
- Complete 10 interviews with new direction
- You still have time
- Pivoting in Week 4 is normal and acceptable

**What NOT to do:**
- ❌ Continue with a problem that's not working "because we already started"
- ❌ Fake interview findings to make the problem seem real
- ❌ Lower your standards for what counts as validation
- ❌ Panic - pivots are normal in product development

---

### The Instructor Conversation

**When to talk to instructor:**
- After 3-5 interviews if seeing multiple red flags
- Before deciding to pivot
- When unsure if concerns are valid

**What to bring to conversation:**
- Summary of interviews so far
- Specific red flags you're seeing
- Your thoughts on pivot vs continue
- Alternative problem ideas if pivoting

**Instructor will help you:**
- Assess if concerns are valid
- Decide if pivot is right move
- Identify if ICP adjustment might help
- Plan next steps

---

## Summary

These frameworks help you:
1. **Select the right problem** using 4 Filters
2. **Define specific ICP** using Specificity Framework
3. **Write quality script** using Quality Rubric
4. **Ensure Mom Test compliance** using Compliance Checker
5. **Make team decisions** using Decision Framework
6. **Prepare for synthesis** using Synthesis Guide
7. **Recognize when to pivot** using Red Flags

**Remember:** Frameworks are tools, not rules. Use your judgment. When in doubt, ask instructor.

---

**Questions?** Email zeshan.ahmad@kiu.edu.ge or visit office hours Tuesday 2-4 PM.
