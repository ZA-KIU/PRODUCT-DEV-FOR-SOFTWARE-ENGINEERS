# Lab 8 Homework: Execute Your First Validation Experiment

**Due Date:** Lab 8 + Lab 9 (TBA) Sprint Due Thursday December 11th 2025 23:59pm 
**Points:** 100  
**Submission:** Via GitHub repository + in-class presentation

---

## üéØ Assignment Overview

You've designed 3 experiments in lab. Now it's time to run one and learn from real data.

**Core Objective:** Execute your highest-priority experiment, collect meaningful data, and make an evidence-based decision about your next steps.

---

## üìã Deliverables

All files go in your capstone repository:

### Required Files

1. **Experiment Execution Log** (40 points)
   - File: `03-build/experiments/experiment-logs/experiment-01-results.md`
   - Template provided in `/templates/`

2. **Data Analysis** (30 points)
   - File: `03-build/experiments/experiment-01-analysis.md`
   - Include raw data, charts, and interpretation

3. **Decision Document** (20 points)
   - File: `03-build/experiments/pivot-decision-001.md`
   - Clear pivot/persevere decision with rationale

4. **Updated Roadmap** (10 points)
   - File: `03-build/experiments/updated-roadmap.md`
   - Show how learnings changed your plans

---

## Part 1: Execute Your Experiment (40 points)

### What to Do

1. **Select your experiment**
   - Choose ONE from your experiment plan (usually your #1 riskiest)
   - Must be completable in 1 week
   - Must generate measurable data

2. **Execute the experiment**
   - Follow your experiment design from lab
   - Document every step (see template)
   - Track all metrics in real-time

3. **Document the process**
   - What actually happened vs. plan
   - Any adjustments made and why
   - Obstacles encountered and how you handled them

### Experiment Execution Log Requirements

Your `experiment-01-results.md` must include:

#### Setup (5 points)
- ‚úÖ Date experiment started
- ‚úÖ Exact hypothesis tested
- ‚úÖ Resources/tools used
- ‚úÖ Any deviations from original plan

#### Execution (15 points)
- ‚úÖ Day-by-day activity log
- ‚úÖ Number of participants/interactions
- ‚úÖ Screenshots/evidence of experiment running
- ‚úÖ Any issues and how you resolved them

#### Raw Data (15 points)
- ‚úÖ Complete dataset (CSV or table)
- ‚úÖ All metrics tracked
- ‚úÖ Sample size achieved
- ‚úÖ Data collection methodology

#### Qualitative Insights (5 points)
- ‚úÖ Surprising observations
- ‚úÖ User quotes or feedback
- ‚úÖ Patterns noticed

**Grading Criteria:**
- **36-40 pts (Excellent):** Complete documentation, data exceeds minimum sample size, clear methodology
- **32-35 pts (Good):** Complete documentation, meets minimum sample size, minor gaps
- **28-31 pts (Acceptable):** Some documentation gaps, below minimum sample size, or unclear methodology
- **Below 28:** Incomplete execution or insufficient data

---

## Part 2: Analyze Your Data (30 points)

### What to Do

Create `experiment-01-analysis.md` that includes:

1. **Quantitative Analysis**
   - Compare results to success criteria
   - Statistical significance (if applicable)
   - Trend analysis
   - Visualizations (charts/graphs)

2. **Qualitative Analysis**
   - Themes from user feedback
   - Unexpected behaviors
   - Context that numbers don't capture

3. **Comparison to Assumptions**
   - What was validated
   - What was invalidated
   - What's still uncertain

### Required Analysis Elements

#### Results Summary (10 points)
```markdown
## Results Summary

### Hypothesis
[Exact hypothesis tested]

### Success Criteria
- Primary metric: [metric] | Target: [X] | Actual: [Y]
- Secondary metric: [metric] | Target: [X] | Actual: [Y]

### Outcome
- [ ] Hypothesis validated
- [ ] Hypothesis invalidated
- [ ] Inconclusive - need more data
```

#### Data Visualization (10 points)
- At least 2 charts/graphs
- Clear labels and legends
- Comparison to benchmarks
- Time-series data if applicable

#### Key Learnings (10 points)
- Top 3 insights
- Surprises (what you didn't expect)
- Implications for product direction

**Grading Criteria:**
- **27-30 pts (Excellent):** Thorough analysis with visualizations, clear insights, ties to product strategy
- **24-26 pts (Good):** Solid analysis, adequate visualizations, good insights
- **21-23 pts (Acceptable):** Basic analysis, minimal visualizations, surface-level insights
- **Below 21:** Incomplete or superficial analysis

---

## Part 3: Make a Decision (20 points)

### What to Do

Create `pivot-decision-001.md` documenting your decision.

### Decision Framework

Based on your results, choose one:

**1. PERSEVERE** (hypothesis validated)
- Continue with current approach
- Build the next feature
- Scale the experiment

**2. PIVOT** (hypothesis invalidated)
- Change customer segment
- Change problem focus
- Change solution approach
- Change value proposition

**3. ITERATE** (inconclusive)
- Redesign experiment
- Get more data
- Test related assumption

### Required Decision Document

```markdown
# Pivot Decision #001

## Date
[Date of decision]

## Team Members Present
- [Name]
- [Name]
- [Name]

## Decision
[PERSEVERE / PIVOT / ITERATE]

## Rationale

### What the Data Showed
[3-5 bullet points of key findings]

### Why This Decision
[Detailed reasoning - minimum 200 words]

### What We're Changing (if pivot/iterate)
- **Before:** [Old approach]
- **After:** [New approach]
- **Why:** [Reasoning]

### What We're Keeping (even if pivoting)
[What's still valid]

## Impact on Roadmap

### Features Affected
- [Feature 1]: [Deprioritized / Changed / Cancelled]
- [Feature 2]: [Deprioritized / Changed / Cancelled]

### New Priorities
1. [Priority 1]
2. [Priority 2]
3. [Priority 3]

## Next Steps
- [ ] [Concrete action 1]
- [ ] [Concrete action 2]
- [ ] [Concrete action 3]

## Timeline
- Next experiment: [Date]
- Next milestone: [Date]

## Risks of This Decision
1. **Risk:** [What could go wrong]
   **Mitigation:** [How we'll handle it]

2. **Risk:** [What could go wrong]
   **Mitigation:** [How we'll handle it]
```

**Grading Criteria:**
- **18-20 pts (Excellent):** Clear decision backed by data, thorough reasoning, concrete next steps
- **16-17 pts (Good):** Decision made with adequate justification, reasonable next steps
- **14-15 pts (Acceptable):** Decision made but weak justification or vague next steps
- **Below 14:** No clear decision or not backed by data

---

## Part 4: Update Your Roadmap (10 points)

### What to Do

Create `updated-roadmap.md` showing how learnings changed your plans.

### Required Elements

```markdown
# Updated Product Roadmap
**Last Updated:** [Date]
**Changes Based On:** Experiment 01 results

## Changes from Original Roadmap

### Original Plan (from Lab 6)
[Bullet points of original 4-6 week plan]

### What Changed and Why
- **Changed:** [What]
- **Reason:** [Why based on experiment]
- **New approach:** [What now]

## Next 4 Weeks (Revised)

### Week 1
- **Focus:** [What you're building/testing]
- **Deliverables:** [Specific outputs]
- **Success criteria:** [How you'll measure]

### Week 2
- **Focus:** [What you're building/testing]
- **Deliverables:** [Specific outputs]
- **Success criteria:** [How you'll measure]

### Week 3
- **Focus:** [What you're building/testing]
- **Deliverables:** [Specific outputs]
- **Success criteria:** [How you'll measure]

### Week 4
- **Focus:** [What you're building/testing]
- **Deliverables:** [Specific outputs]
- **Success criteria:** [How you'll measure]

## Deferred / Descoped
[Features that are now lower priority or cut]

## New Assumptions to Test
[What you need to validate next]
```

**Grading Criteria:**
- **9-10 pts (Excellent):** Clear changes, well-justified priorities, specific milestones
- **8 pts (Good):** Changes documented, reasonable priorities
- **7 pts (Acceptable):** Basic roadmap updated
- **Below 7:** Incomplete or no changes documented

---

## üìä Grading Rubric Summary

| Component | Points | Key Criteria |
|-----------|--------|--------------|
| **Experiment Execution** | 40 | Complete data, proper methodology, adequate sample size |
| **Data Analysis** | 30 | Thorough analysis, visualizations, clear insights |
| **Decision Document** | 20 | Clear decision backed by data, concrete next steps |
| **Updated Roadmap** | 10 | Shows learning integration, realistic priorities |
| **TOTAL** | **100** | |

---

## üéØ Success Criteria

### Minimum Requirements (to pass)
- ‚úÖ Experiment executed with minimum 10 data points
- ‚úÖ Results documented with basic analysis
- ‚úÖ Decision made (persevere/pivot/iterate)
- ‚úÖ Roadmap updated

### Excellent Work (90-100 points)
- ‚úÖ Exceeds minimum sample size (20+ data points)
- ‚úÖ Statistical analysis included
- ‚úÖ Visualizations enhance understanding
- ‚úÖ Deep insights about user behavior
- ‚úÖ Clear connection between data and decisions
- ‚úÖ Specific, actionable next steps

### Red Flags (will lose points)
- ‚ùå Claiming results without data
- ‚ùå Ignoring data that contradicts assumptions
- ‚ùå Vague or generic analysis
- ‚ùå Decision not based on experiment results
- ‚ùå No change to roadmap despite learnings

---

## üì¶ Submission Requirements

### GitHub Repository
1. **Commit all files** to your capstone repo
2. **Folder structure:**
   ```
   03-build/
   ‚îú‚îÄ‚îÄ experiments/
   ‚îÇ   ‚îú‚îÄ‚îÄ hypothesis-prioritization.md (from lab)
   ‚îÇ   ‚îú‚îÄ‚îÄ experiment-plan.md (from lab)
   ‚îÇ   ‚îú‚îÄ‚îÄ experiment-logs/
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ experiment-01-results.md ‚Üê HOMEWORK
   ‚îÇ   ‚îú‚îÄ‚îÄ experiment-01-analysis.md ‚Üê HOMEWORK
   ‚îÇ   ‚îú‚îÄ‚îÄ pivot-decision-001.md ‚Üê HOMEWORK
   ‚îÇ   ‚îî‚îÄ‚îÄ updated-roadmap.md ‚Üê HOMEWORK
   ‚îî‚îÄ‚îÄ validation/
       ‚îî‚îÄ‚îÄ ... (from lab)
   ```

3. **Commit message:** "Lab 8 Homework: Experiment 01 complete - [Persevere/Pivot/Iterate]"

4. **Submit:** Push to GitHub and share link via submission form

### In-Class Presentation (Week 11)
- **Duration:** 5 minutes per team
- **Slides:** 3-5 slides maximum
- **Content:**
  1. What you tested
  2. What you learned
  3. Decision made
  4. What's next

**Presentation template provided separately**

---

## üí° Tips for Success

### Do's
‚úÖ Start experiment early in the week (Monday/Tuesday)
‚úÖ Track data daily - don't wait until the end
‚úÖ Be honest about negative results (they're still learning)
‚úÖ Talk to users, not just collect clicks
‚úÖ Document as you go, not at the end

### Don'ts
‚ùå Don't fake data or inflate numbers
‚ùå Don't ignore results that contradict your assumptions
‚ùå Don't skip the analysis - raw data isn't enough
‚ùå Don't change experiment mid-way without documenting why
‚ùå Don't wait until last minute to start

### If Things Go Wrong
- **Can't recruit enough users?** Document why, try alternative channels, smaller sample is okay if you explain limitations
- **Technical issues?** Document the problem, show your workaround, explain impact on results
- **Results inconclusive?** That's okay! Document what you learned and design follow-up experiment
- **Complete failure?** GREAT! Document learnings and pivot. Failure with learning is success in this class.

---

## üÜò Getting Help

### Office Hours
**When:** Tuesdays 2-4 PM  
**Where:** [Location]  
**Bring:** Your experiment plan and any early data

### Slack Channel
`#lab-8-questions`  
Post questions, share progress, help each other

### Common Issues

**Q: Our experiment isn't getting enough participants**
A: Lower your sample size requirement (with justification), try new channels, or pivot to different experiment type

**Q: Results contradict what we expected**
A: Perfect! That's the point. Document the surprise and what it means.

**Q: We don't know if results are "good enough"**
A: Compare to your success criteria from lab. If you hit the threshold, it's validated. If not, it's not. Document either way.

**Q: Can we change our experiment mid-way?**
A: Only if you have a very good reason. Document the change and rationale clearly.

---

## üìö Additional Resources

### Required Reading
- "The Lean Startup" Chapter 6: Test - Eric Ries
- "Hypothesis-Driven Development" - Barry O'Reilly

### Optional But Helpful
- "The Mom Test" chapters on testing (refresh from Week 2)
- "Running Lean" by Ash Maurya - validation chapters
- Steve Blank "4 Steps to the Epiphany" - customer validation

### Tools
- **Landing pages:** Carrd, Webflow, Typedream
- **Analytics:** Mixpanel (free tier), Amplitude (free tier)
- **Surveys:** Google Forms, Typeform
- **Email:** Mailchimp (free tier), Buttondown

---

## üéØ Learning Objectives

After completing this homework, you will have:

‚úÖ Real data about your product assumptions  
‚úÖ Experience running lean experiments  
‚úÖ Practice making evidence-based decisions  
‚úÖ Refined product direction based on learning  
‚úÖ Muscle memory for the Build-Measure-Learn cycle  

**This is where theory becomes practice. Make it count!**

---

## üöÄ What's Next: Lab 9

After completing your validation experiments, you'll move into financial modeling:

**Lab 9 (Week 11): Unit Economics & Financial Modeling**
- Calculate LTV (Lifetime Value) and CAC (Customer Acquisition Cost)
- Build 12-month financial projections
- Validate business model viability
- Connect your validated assumptions to unit economics

Your validation learnings from Lab 8 will directly inform your financial assumptions in Lab 9.

---

**Remember:** The goal isn't to prove you're right. The goal is to learn fast and build something people actually want.

Good luck! üöÄ
