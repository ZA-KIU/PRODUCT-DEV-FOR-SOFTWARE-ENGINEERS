# Lab 8 Homework: Execute Your First Validation Experiment

**Due Date:** Lab 8 + Lab 9 (TBA) Sprint Due Thursday December 11th 2025 23:59pm 
**Points:** 100  
**Submission:** Via GitHub repository + in-class presentation

---

## üéØ Assignment Overview

You've designed 3 experiments in lab. Now it's time to run one and learn from real data.

**Core Objective:** Execute your highest-priority experiment, collect meaningful data, and make an evidence-based decision about your next steps.

---

## üìã Deliverables

All files go in your capstone repository:

### Required Files

1. **Experiment Execution Log** (40 points)
   - File: `03-build/experiments/experiment-logs/experiment-01-results.md`
   - Template provided in `/templates/`

2. **Data Analysis** (30 points)
   - File: `03-build/experiments/experiment-01-analysis.md`
   - Include raw data, charts, and interpretation

3. **Decision Document** (20 points)
   - File: `03-build/experiments/pivot-decision-001.md`
   - Clear pivot/persevere decision with rationale

4. **Updated Roadmap** (10 points)
   - File: `03-build/experiments/updated-roadmap.md`
   - Show how learnings changed your plans

---

## Part 1: Execute Your Experiment (40 points)

### What to Do

1. **Select your experiment**
   - Choose ONE from your experiment plan (usually your #1 riskiest)
   - Must be completable in 1 week
   - Must generate measurable data

2. **Execute the experiment**
   - Follow your experiment design from lab
   - Document every step (see template)
   - Track all metrics in real-time

3. **Document the process**
   - What actually happened vs. plan
   - Any adjustments made and why
   - Obstacles encountered and how you handled them

### Experiment Execution Log Requirements

Your `experiment-01-results.md` must include:

#### Setup (5 points)
- ‚úÖ Date experiment started
- ‚úÖ Exact hypothesis tested
- ‚úÖ Resources/tools used
- ‚úÖ Any deviations from original plan

#### Execution (15 points)
- ‚úÖ Day-by-day activity log
- ‚úÖ Number of participants/interactions
- ‚úÖ Screenshots/evidence of experiment running
- ‚úÖ Any issues and how you resolved them

#### Raw Data (15 points)
- ‚úÖ Complete dataset (CSV or table)
- ‚úÖ All metrics tracked
- ‚úÖ Sample size achieved
- ‚úÖ Data collection methodology

#### Qualitative Insights (5 points)
- ‚úÖ Surprising observations
- ‚úÖ User quotes or feedback
- ‚úÖ Patterns noticed

**Grading Criteria:**
- **36-40 pts (Excellent):** Complete documentation, data exceeds minimum sample size, clear methodology
- **32-35 pts (Good):** Complete documentation, meets minimum sample size, minor gaps
- **28-31 pts (Acceptable):** Some documentation gaps, below minimum sample size, or unclear methodology
- **Below 28:** Incomplete execution or insufficient data

---

## Part 2: Analyze Your Data (30 points)

### What to Do

Create `experiment-01-analysis.md` that includes:

1. **Quantitative Analysis**
   - Compare results to success criteria
   - Statistical significance (if applicable)
   - Trend analysis
   - Visualizations (charts/graphs)

2. **Qualitative Analysis**
   - Themes from user feedback
   - Unexpected behaviors
   - Context that numbers don't capture

3. **Comparison to Assumptions**
   - What was validated
   - What was invalidated
   - What's still uncertain

### Required Analysis Elements

#### Results Summary (10 points)
```markdown
## Results Summary

### Hypothesis
[Exact hypothesis tested]

### Success Criteria
- Primary metric: [metric] | Target: [X] | Actual: [Y]
- Secondary metric: [metric] | Target: [X] | Actual: [Y]

### Outcome
- [ ] Hypothesis validated
- [ ] Hypothesis invalidated
- [ ] Inconclusive - need more data
```

#### Data Visualization (10 points)
- At least 2 charts/graphs
- Clear labels and legends
- Comparison to benchmarks
- Time-series data if applicable

#### Key Learnings (10 points)
- Top 3 insights
- Surprises (what you didn't expect)
- Implications for product direction

**Grading Criteria:**
- **27-30 pts (Excellent):** Thorough analysis with visualizations, clear insights, ties to product strategy
- **24-26 pts (Good):** Solid analysis, adequate visualizations, good insights
- **21-23 pts (Acceptable):** Basic analysis, minimal visualizations, surface-level insights
- **Below 21:** Incomplete or superficial analysis

---

## Part 3: Make a Decision (20 points)

### What to Do

Create `pivot-decision-001.md` documenting your decision.

### Decision Framework

Based on your results, choose one:

**1. PERSEVERE** (hypothesis validated)
- Continue with current approach
- Build the next feature
- Scale the experiment

**2. PIVOT** (hypothesis invalidated)
- Change customer segment
- Change problem focus
- Change solution approach
- Change value proposition

**3. ITERATE** (inconclusive)
- Redesign experiment
- Get more data
- Test related assumption

### Required Decision Document

```markdown
# Pivot Decision #001

## Date
[Date of decision]

## Team Members Present
- [Name]
- [Name]
- [Name]

## Decision
[PERSEVERE / PIVOT / ITERATE]

## Rationale

### What the Data Showed
[3-5 bullet points of key findings]

### Why This Decision
[Detailed reasoning - minimum 200 words]

### What We're Changing (if pivot/iterate)
- **Before:** [Old approach]
- **After:** [New approach]
- **Why:** [Reasoning]

### What We're Keeping (even if pivoting)
[What's still valid]

## Impact on Roadmap

### Features Affected
- [Feature 1]: [Deprioritized / Changed / Cancelled]
- [Feature 2]: [Deprioritized / Changed / Cancelled]

### New Priorities
1. [Priority 1]
2. [Priority 2]
3. [Priority 3]

## Next Steps
- [ ] [Concrete action 1]
- [ ] [Concrete action 2]
- [ ] [Concrete action 3]

## Timeline
- Next experiment: [Date]
- Next milestone: [Date]

## Risks of This Decision
1. **Risk:** [What could go wrong]
   **Mitigation:** [How we'll handle it]

2. **Risk:** [What could go wrong]
   **Mitigation:** [How we'll handle it]
```

**Grading Criteria:**
- **18-20 pts (Excellent):** Clear decision backed by data, thorough reasoning, concrete next steps
- **16-17 pts (Good):** Decision made with adequate justification, reasonable next steps
- **14-15 pts (Acceptable):** Decision made but weak justification or vague next steps
- **Below 14:** No clear decision or not backed by data

---

## Part 4: Update Your Roadmap (10 points)

### What to Do

Create `updated-roadmap.md` showing how learnings changed your plans.

### Required Elements

```markdown
# Updated Product Roadmap
**Last Updated:** [Date]
**Changes Based On:** Experiment 01 results

## Changes from Original Roadmap

### Original Plan (from Lab 6)
[Bullet points of original 4-6 week plan]

### What Changed and Why
- **Changed:** [What]
- **Reason:** [Why based on experiment]
- **New approach:** [What now]

## Next 4 Weeks (Revised)

### Week 1
- **Focus:** [What you're building/testing]
- **Deliverables:** [Specific outputs]
- **Success criteria:** [How you'll measure]

### Week 2
- **Focus:** [What you're building/testing]
- **Deliverables:** [Specific outputs]
- **Success criteria:** [How you'll measure]

### Week 3
- **Focus:** [What you're building/testing]
- **Deliverables:** [Specific outputs]
- **Success criteria:** [How you'll measure]

### Week 4
- **Focus:** [What you're building/testing]
- **Deliverables:** [Specific outputs]
- **Success criteria:** [How you'll measure]

## Deferred / Descoped
[Features that are now lower priority or cut]

## New Assumptions to Test
[What you need to validate next]
```

**Grading Criteria:**
- **9-10 pts (Excellent):** Clear changes, well-justified priorities, specific milestones
- **8 pts (Good):** Changes documented, reasonable priorities
- **7 pts (Acceptable):** Basic roadmap updated
- **Below 7:** Incomplete or no changes documented

---

## üìä Grading Rubric Summary

| Component | Points | Key Criteria |
|-----------|--------|--------------|
| **Experiment Execution** | 40 | Complete data, proper methodology, adequate sample size |
| **Data Analysis** | 30 | Thorough analysis, visualizations, clear insights |
| **Decision Document** | 20 | Clear decision backed by data, concrete next steps |
| **Updated Roadmap** | 10 | Shows learning integration, realistic priorities |
| **TOTAL** | **100** | |

---

## üéØ Success Criteria

### Minimum Requirements (to pass)
- ‚úÖ Experiment executed with minimum 10 data points
- ‚úÖ Results documented with basic analysis
- ‚úÖ Decision made (persevere/pivot/iterate)
- ‚úÖ Roadmap updated

### Excellent Work (90-100 points)
- ‚úÖ Exceeds minimum sample size (20+ data points)
- ‚úÖ Statistical analysis included
- ‚úÖ Visualizations enhance understanding
- ‚úÖ Deep insights about user behavior
- ‚úÖ Clear connection between data and decisions
- ‚úÖ Specific, actionable next steps

### Red Flags (will lose points)
- ‚ùå Claiming results without data
- ‚ùå Ignoring data that contradicts assumptions
- ‚ùå Vague or generic analysis
- ‚ùå Decision not based on experiment results
- ‚ùå No change to roadmap despite learnings

---

## üì¶ Submission Requirements

### GitHub Repository
1. **Commit all files** to your capstone repo
2. **Folder structure:**
   ```
   03-build/
   ‚îú‚îÄ‚îÄ experiments/
   ‚îÇ   ‚îú‚îÄ‚îÄ hypothesis-prioritization.md (from lab)
   ‚îÇ   ‚îú‚îÄ‚îÄ experiment-plan.md (from lab)
   ‚îÇ   ‚îú‚îÄ‚îÄ experiment-logs/
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ experiment-01-results.md ‚Üê HOMEWORK
   ‚îÇ   ‚îú‚îÄ‚îÄ experiment-01-analysis.md ‚Üê HOMEWORK
   ‚îÇ   ‚îú‚îÄ‚îÄ pivot-decision-001.md ‚Üê HOMEWORK
   ‚îÇ   ‚îî‚îÄ‚îÄ updated-roadmap.md ‚Üê HOMEWORK
   ‚îî‚îÄ‚îÄ validation/
       ‚îî‚îÄ‚îÄ ... (from lab)
   ```

3. **Commit message:** "Lab 8 Homework: Experiment 01 complete - [Persevere/Pivot/Iterate]"

4. **Submit:** Push to GitHub and share link via submission form

### In-Class Presentation (Week 11)
- **Duration:** 5 minutes per team
- **Slides:** 3-5 slides maximum
- **Content:**
  1. What you tested
  2. What you learned
  3. Decision made
  4. What's next

**Presentation template provided separately**

---

## üí° Tips for Success

### Do's
‚úÖ Start experiment early in the week (Monday/Tuesday)
‚úÖ Track data daily - don't wait until the end
‚úÖ Be honest about negative results (they're still learning)
‚úÖ Talk to users, not just collect clicks
‚úÖ Document as you go, not at the end

### Don'ts
‚ùå Don't fake data or inflate numbers
‚ùå Don't ignore results that contradict your assumptions
‚ùå Don't skip the analysis - raw data isn't enough
‚ùå Don't change experiment mid-way without documenting why
‚ùå Don't wait until last minute to start

### If Things Go Wrong
- **Can't recruit enough users?** Document why, try alternative channels, smaller sample is okay if you explain limitations
- **Technical issues?** Document the problem, show your workaround, explain impact on results
- **Results inconclusive?** That's okay! Document what you learned and design follow-up experiment
- **Complete failure?** GREAT! Document learnings and pivot. Failure with learning is success in this class.

---

## üÜò Getting Help

### Office Hours
**When:** Tuesdays 2-4 PM  
**Where:** [Location]  
**Bring:** Your experiment plan and any early data

### Slack Channel
`#lab-8-questions`  
Post questions, share progress, help each other

### Common Issues

**Q: Our experiment isn't getting enough participants**
A: Lower your sample size requirement (with justification), try new channels, or pivot to different experiment type

**Q: Results contradict what we expected**
A: Perfect! That's the point. Document the surprise and what it means.

**Q: We don't know if results are "good enough"**
A: Compare to your success criteria from lab. If you hit the threshold, it's validated. If not, it's not. Document either way.

**Q: Can we change our experiment mid-way?**
A: Only if you have a very good reason. Document the change and rationale clearly.

---

## üìö Additional Resources

### Required Reading
- "The Lean Startup" Chapter 6: Test - Eric Ries
- "Hypothesis-Driven Development" - Barry O'Reilly

### Optional But Helpful
- "The Mom Test" chapters on testing (refresh from Week 2)
- "Running Lean" by Ash Maurya - validation chapters
- Steve Blank "4 Steps to the Epiphany" - customer validation

### Tools
- **Landing pages:** Carrd, Webflow, Typedream
- **Analytics:** Mixpanel (free tier), Amplitude (free tier)
- **Surveys:** Google Forms, Typeform
- **Email:** Mailchimp (free tier), Buttondown

---

## üéØ Learning Objectives

After completing this homework, you will have:

‚úÖ Real data about your product assumptions  
‚úÖ Experience running lean experiments  
‚úÖ Practice making evidence-based decisions  
‚úÖ Refined product direction based on learning  
‚úÖ Muscle memory for the Build-Measure-Learn cycle  

**This is where theory becomes practice. Make it count!**

---

## üöÄ What's Next: Lab 9

After completing your validation experiments, you'll move into financial modeling:

**Lab 9 (Week 11): Unit Economics & Financial Modeling**
- Calculate LTV (Lifetime Value) and CAC (Customer Acquisition Cost)
- Build 12-month financial projections
- Validate business model viability
- Connect your validated assumptions to unit economics

Your validation learnings from Lab 8 will directly inform your financial assumptions in Lab 9.

For Lab 8 here is your Submission Checklist:
# Lab 8 Submission Checklist

Use this to verify you've completed everything before submitting.

---

## ‚úÖ Lab Participation (Due: End of Lab)

### Required Files in Your Repo

- [ ] `03-build/experiments/hypothesis-prioritization.md`
- [ ] `03-build/experiments/experiment-plan.md`
- [ ] `03-build/validation/success-metrics.md`
- [ ] `03-build/validation/smoke-test-plan.md`
- [ ] `03-build/validation/validation-sprint-plan.md`

### Quality Checks

**hypothesis-prioritization.md:**
- [ ] 15+ assumptions listed
- [ ] All assumptions scored (Impact & Confidence)
- [ ] Risk scores calculated correctly
- [ ] Top 3 riskiest identified
- [ ] Full rationale for each top 3

**experiment-plan.md:**
- [ ] 3 complete experiments designed
- [ ] Each has hypothesis statement
- [ ] Each has success criteria with numbers
- [ ] Each has test procedure (4 phases)
- [ ] Timeline and resources documented
- [ ] Potential outcomes mapped

**success-metrics.md:**
- [ ] North star metric defined
- [ ] AARRR metrics specified
- [ ] Targets set for each metric
- [ ] Measurement plan documented

**smoke-test-plan.md:**
- [ ] Test type selected
- [ ] Landing page content drafted
- [ ] Success criteria defined (conversion rate threshold)
- [ ] 3+ distribution channels planned
- [ ] Timeline complete

**validation-sprint-plan.md:**
- [ ] 2-week plan with daily tasks
- [ ] Owners assigned to each task
- [ ] Success criteria defined
- [ ] Risks identified

### Git Commit

- [ ] All files committed
- [ ] Descriptive commit message
- [ ] Pushed to GitHub
- [ ] Verified files visible in GitHub web interface

---

## ‚úÖ Homework (Due: One Week After Lab)

### Required Files in Your Repo

- [ ] `03-build/experiments/experiment-logs/experiment-01-results.md`
- [ ] `03-build/experiments/experiment-01-analysis.md`
- [ ] `03-build/experiments/pivot-decision-001.md`
- [ ] `03-build/experiments/updated-roadmap.md`
- [ ] Raw data file (CSV or spreadsheet)

### Quality Checks

**experiment-01-results.md:**
- [ ] Setup section complete
- [ ] Day-by-day execution log
- [ ] Raw data table included
- [ ] Minimum sample size reached (or explained)
- [ ] 5+ qualitative quotes
- [ ] Observations documented
- [ ] Link to raw data file

**experiment-01-analysis.md:**
- [ ] Results vs. success criteria table
- [ ] 2+ data visualizations (charts)
- [ ] Qualitative themes identified
- [ ] 3+ key learnings documented
- [ ] Implications for product stated
- [ ] Statistical significance assessed

**pivot-decision-001.md:**
- [ ] Clear decision stated (Persevere / Pivot / Iterate)
- [ ] Rationale is 200+ words
- [ ] Decision backed by data from experiment
- [ ] 3+ concrete next steps with owners and dates
- [ ] Risks identified with mitigation
- [ ] All team members signed off

**updated-roadmap.md:**
- [ ] Changes from original roadmap documented
- [ ] Rationale for changes tied to learnings
- [ ] 4-week plan with weekly breakdown
- [ ] Features prioritized/deprioritized with reasoning
- [ ] New assumptions to test identified

**Raw Data:**
- [ ] Complete dataset (all participants/visitors)
- [ ] All metrics tracked
- [ ] Data is clean (no obvious errors)
- [ ] Includes timestamp/date information
- [ ] Linked from results document

### Content Quality

**Execution:**
- [ ] Experiment ran for planned duration
- [ ] Any deviations documented with reasoning
- [ ] Issues encountered documented with resolution
- [ ] Data collection was systematic

**Analysis:**
- [ ] Comparison to success criteria is explicit
- [ ] Visualizations have clear labels
- [ ] Insights connect to specific data points
- [ ] Both quantitative and qualitative analysis
- [ ] Honest about limitations

**Decision:**
- [ ] Decision matches what data shows
- [ ] Not ignoring contradictory evidence
- [ ] Next steps are specific and actionable
- [ ] Roadmap changes reflect learnings

### Git Commit

- [ ] All files committed
- [ ] Raw data file committed
- [ ] Commit message format: "Lab 8 Homework: Experiment 01 - [Decision]"
- [ ] Pushed to GitHub
- [ ] All files accessible via GitHub web interface

---

## ‚úÖ Common Mistakes to Avoid

### Execution Phase

‚ùå Starting experiment Friday night (too late!)  
‚ùå Not tracking data daily  
‚ùå Changing experiment without documenting  
‚ùå Insufficient sample size without explanation  
‚ùå No qualitative data collected  

### Analysis Phase

‚ùå No comparison to success criteria  
‚ùå Ignoring data that contradicts assumptions  
‚ùå Generic insights ("users want features")  
‚ùå No visualizations  
‚ùå Not connecting insights to decisions  

### Decision Phase

‚ùå Claiming validation when missed success criteria  
‚ùå Pivoting based on opinion not data  
‚ùå Vague next steps ("improve the product")  
‚ùå No changes to roadmap despite learnings  
‚ùå Not all team members agreeing/signing off  

---

## ‚úÖ Before Final Submission

### Double-Check

- [ ] All filenames match exactly (case-sensitive)
- [ ] All templates filled out completely
- [ ] No [placeholder text] remaining
- [ ] Links work (to data files, dashboards)
- [ ] Images/charts display correctly
- [ ] Markdown formatting renders properly in GitHub

### Team Review

- [ ] All team members reviewed all documents
- [ ] Team discussed and agreed on decision
- [ ] Everyone signed pivot-decision document
- [ ] Responsibilities for next steps assigned
- [ ] Team calendar updated with next milestones

### Final Git Check

```bash
# Verify all files present
git status

# Check file structure
tree 03-build/

# Verify nothing staged but not committed
git diff --cached

# Push to GitHub
git push origin main

# Verify on GitHub web interface
# Open: https://github.com/[your-repo]/tree/main/03-build
```

---

## ‚úÖ Submission

### Via Class Form

- [ ] Fill out submission form with:
  - Team name
  - GitHub repository URL
  - Experiment decision (Persevere/Pivot/Iterate)
  - Key learning (one sentence)
  - Any issues or blockers

### Verification

- [ ] Clicked GitHub link in form to verify it works
- [ ] Checked that strangers can access your public repo
- [ ] Verified latest commit shows all homework files
- [ ] Screenshots taken as backup

---

## ‚úÖ Presentation Prep (If Required)

### 5-Minute Presentation

**Slide 1: What We Tested**
- Hypothesis in one sentence
- Why it was risky

**Slide 2: How We Tested**
- Experiment method
- Success criteria
- Sample size

**Slide 3: What We Found**
- Results vs. target
- Key insight
- Surprising finding

**Slide 4: What We're Doing**
- Decision (Persevere/Pivot/Iterate)
- Why
- What's changing

**Slide 5: What's Next**
- Next 2 weeks plan
- Next experiment
- Ask for feedback

### Practice

- [ ] Timed to 5 minutes
- [ ] Everyone speaks
- [ ] Transitions smooth
- [ ] Can answer questions
- [ ] Backup slides ready (detailed data)

---

## üéØ Final Verification

**Ask yourself:**

‚úÖ **Can someone else understand our experiment without explanation?**

‚úÖ **Did we test what we said we'd test?**

‚úÖ **Is our decision backed by the data in our documents?**

‚úÖ **Would a stranger looking at our repo know what we did and why?**

‚úÖ **Are we being honest about what we learned (including negatives)?**

If yes to all ‚Üí You're ready to submit!

---

## üÜò Last-Minute Issues?

### If you're missing data:
- Document what happened and why
- Explain limitations honestly
- State what you'd do differently
- Don't fabricate numbers

### If results are unclear:
- That's okay! Say "inconclusive"
- Explain why (sample size, methodology, etc.)
- Propose follow-up experiment
- Document what you learned anyway

### If you need to resubmit:
- Check late policy (-10% per 24hrs)
- Fix issues documented in feedback
- Recommit with clear message
- Resubmit via form with "RESUBMISSION" note

---

**Remember:** Learning from failure is success in this class. Be honest about your results!

Good luck! üöÄ

---

**Remember:** The goal isn't to prove you're right. The goal is to learn fast and build something people actually want.

Good luck! üöÄ
